{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0546c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import glob\n",
    "import textstat\n",
    "import re\n",
    "import math\n",
    "import demoji\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import collections\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70981397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(data, unit='natural'): #entropy\n",
    "    base = {\n",
    "        'shannon' : 2.,\n",
    "        'natural' : math.exp(1),\n",
    "        'hartley' : 10.\n",
    "    }\n",
    "\n",
    "    if len(data) <= 1:\n",
    "        return 0\n",
    "\n",
    "    counts = Counter()\n",
    "\n",
    "    for d in data:\n",
    "        counts[d] += 1\n",
    "\n",
    "    ent = 0\n",
    "\n",
    "    probs = [float(c) / len(data) for c in counts.values()]\n",
    "    for p in probs:\n",
    "        if p > 0.:\n",
    "            ent -= p * math.log(p, base[unit])\n",
    "\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0568cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment(text):\n",
    "    clean_text = text.lower() #lowercase\n",
    "    clean_text = re.sub(r\"<[^>]+>\", '', clean_text, flags=re.MULTILINE)\n",
    "    clean_text = re.sub(r\"&[^;]+;\", '', clean_text, flags=re.MULTILINE)\n",
    "    clean_text = re.sub(r\"\\n\", ' ', clean_text, flags=re.MULTILINE)\n",
    "    clean_text = demoji.replace_with_desc(clean_text)\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"spanish\")) | set(stopwords.words(\"dutch\")) | set(stopwords.words(\"french\")) | set(stopwords.words(\"german\")) | set(stopwords.words(\"italian\")) | set(stopwords.words(\"portuguese\")) | set(stopwords.words(\"romanian\"))\n",
    "    \n",
    "    words = clean_text.split(\" \")\n",
    "    filtered_words = [word for word in words if word not in stops and word.isalpha()]\n",
    "    clean_text = ' '.join(filtered_words)\n",
    "    \n",
    "    clean_text = ''.join([y for y in clean_text if str.isalnum(y) or y == \" \"])\n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f476e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant_user(occupation):\n",
    "    words = [\"fot\",\"phot\",\"valokuv\",\"zdjęcie\",\"dealbh\",\"bild\",\"grianghraf\", \"nuotrauk\",\"pictur\",\"myndin\",\"billed\",\"ljósmyndari\",\"ritratt\"]\n",
    "    return any(w in occupation.lower() for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b654dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['nsid', 'ispro', 'occupation', 'following_n', 'photo_count', 'join_date', 'website', 'profile_description', 'groups', 'groups_n', 'is_photographer', 'following'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('user_features.json',\"r\", encoding=\"utf8\") as f:\n",
    "    user_features = json.load(f)\n",
    "user_features[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8017109",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in user_features:\n",
    "    user['is_photographer'] = is_relevant_user(user['occupation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d42cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save a new file with one additional secondary feature for users\n",
    "with open('user_features_secondary.json', \"w\") as outfile:\n",
    "    json.dump(user_features,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d09783dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['nsid', 'ispro', 'occupation', 'following_n', 'photo_count', 'join_date', 'website', 'profile_description', 'groups', 'groups_n', 'is_photographer', 'following'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('user_features_secondary.json',\"r\", encoding=\"utf8\") as f:\n",
    "    user_features = json.load(f)\n",
    "user_features[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947b086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'owner', 'title', 'description', 'views', 'dateuploaded', 'lastupdate', 'tags', 'comments', 'favorites', 'exif', 'groups', 'width_o', 'height_o', 'width_downloaded', 'height_downloaded', 'kong_score', 'nima_score', 'nima_tech_score', 'comments_n', 'favorites_n', 'groups_n'])\n",
      "2647927\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for filename in glob.glob('photo_features\\\\*'):\n",
    "    #simple_name = filename.split(\"\\\\\")[-1][:-5]\n",
    "    with open(filename,\"r\", encoding=\"utf8\") as f:\n",
    "        data.extend(json.load(f))\n",
    "print(data[0].keys())\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comments = 0\n",
    "n_final_comments = 0\n",
    "for row_by_img in data:\n",
    "    \n",
    "    #delete empty comments\n",
    "    filtered_comments = []\n",
    "    n_comments += len(row_by_img[\"comments\"])\n",
    "    row_by_img[\"comments\"] = [c for c in row_by_img[\"comments\"] if c[\"comment\"]]\n",
    "    for comment in row_by_img[\"comments\"]:        \n",
    "        comment[\"comment\"] = clean_comment(comment[\"comment\"])\n",
    "        comment_text = comment[\"comment\"]\n",
    "        if comment_text:\n",
    "            filtered_comments.append(comment)\n",
    "    row_by_img[\"comments\"] = filtered_comments\n",
    "    n_final_comments += len(filtered_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "259ace23",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_subj_pola = 0.5\n",
    "default_read = 0\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "for x in data:\n",
    "    comments = [y[\"comment\"] for y in x[\"comments\"] if y[\"comment\"]]\n",
    "    avg_subj = default_subj_pola\n",
    "    avg_diff_words = default_read\n",
    "    avg_read_time = default_read\n",
    "    avg_entropy = default_read\n",
    "    avg_length = default_read\n",
    "    avg_polarity = default_read\n",
    "    sum_subj = 0\n",
    "    sum_diff_words = 0\n",
    "    sum_read_time = 0\n",
    "    sum_entropy = 0\n",
    "    sum_length = 0\n",
    "    sum_polarity = 0\n",
    "    \n",
    "    for comment in comments:\n",
    "        testimonial = TextBlob(comment)\n",
    "        sum_subj += testimonial.sentiment.subjectivity\n",
    "        sum_diff_words += textstat.difficult_words(comment)\n",
    "        sum_read_time += textstat.reading_time(comment, ms_per_char=14.69)\n",
    "        sum_entropy += compute_entropy(comment)\n",
    "        sum_length += len(comment)\n",
    "        sum_polarity += sia.polarity_scores(comment).get(\"compound\")\n",
    "\n",
    "    if comments:\n",
    "        avg_subj = sum_subj / len(comments)\n",
    "        avg_diff_words = sum_diff_words / len(comments)\n",
    "        avg_read_time = sum_read_time / len(comments)\n",
    "        avg_entropy = sum_entropy / len(comments)\n",
    "        avg_length = sum_length / len(comments)\n",
    "        avg_polarity = sum_polarity / len(comments)\n",
    "        \n",
    "    x[\"avg_subj\"] = avg_subj\n",
    "    x[\"avg_diff_words\"] = avg_diff_words\n",
    "    x[\"avg_read_time\"] = avg_read_time\n",
    "    x[\"avg_entropy\"] = avg_entropy\n",
    "    x[\"avg_length\"] = avg_length\n",
    "    x[\"avg_polarity\"] = avg_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3c99ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save new files with secondary features for photos\n",
    "photos_by_user = defaultdict(list)\n",
    "for x in data:\n",
    "    photos_by_user[x[\"owner\"]].append(x)\n",
    "    \n",
    "for user, rows in photos_by_user.items():\n",
    "    path = f'photo_features_secondary\\\\{user}.json'\n",
    "    with open(path,\"w\") as outfile:\n",
    "        json.dump(rows,outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
